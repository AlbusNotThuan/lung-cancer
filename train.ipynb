{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c92b3e",
   "metadata": {},
   "source": [
    "# Modular Training Pipeline\n",
    "\n",
    "This notebook trains multiple ML models with configurable imbalance handling methods in parallel.\n",
    "\n",
    "**Features:**\n",
    "- 11 ML models supported\n",
    "- 6 imbalance handling methods\n",
    "- Parallel training (n_workers configurable)\n",
    "- Binary and multiclass classification\n",
    "- Automatic result tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f459188",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from datasets import *\n",
    "from modules import *\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4eae63",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Result Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run_folder():\n",
    "    \"\"\"Create timestamped run folder\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_folder = f\"results/run_{timestamp}\"\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    os.makedirs(f\"{run_folder}/plots\", exist_ok=True)\n",
    "    return run_folder\n",
    "\n",
    "def save_summary(run_folder, config, metrics_df, dataset_info):\n",
    "    \"\"\"Save quick summary JSON\"\"\"\n",
    "    best_idx = metrics_df['auc'].idxmax()\n",
    "    \n",
    "    summary = {\n",
    "        \"experiment\": config['experiment'],\n",
    "        \"dataset\": dataset_info,\n",
    "        \"models\": config['models']['active'],\n",
    "        \"imbalance_methods\": config['imbalance']['methods'],\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"best_result\": {\n",
    "            \"job\": f\"{metrics_df.loc[best_idx, 'model']}_{metrics_df.loc[best_idx, 'imbalance_method']}\",\n",
    "            \"model\": metrics_df.loc[best_idx, 'model'],\n",
    "            \"imbalance_method\": metrics_df.loc[best_idx, 'imbalance_method'],\n",
    "            \"auc\": float(metrics_df.loc[best_idx, 'auc']),\n",
    "            \"accuracy\": float(metrics_df.loc[best_idx, 'accuracy'])\n",
    "        },\n",
    "        \"config\": config\n",
    "    }\n",
    "    \n",
    "    with open(f\"{run_folder}/summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "def save_metrics(run_folder, metrics_df):\n",
    "    \"\"\"Save detailed metrics CSV\"\"\"\n",
    "    metrics_df.to_csv(f\"{run_folder}/metrics.csv\", index=False)\n",
    "\n",
    "def save_training_log(run_folder, log_messages):\n",
    "    \"\"\"Save raw training logs\"\"\"\n",
    "    with open(f\"{run_folder}/training.log\", 'w') as f:\n",
    "        f.write('\\n'.join(log_messages))\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a492b56a",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"  Experiment: {config['experiment']['name']}\")\n",
    "print(f\"  Dataset: {config['dataset']['name']}\")\n",
    "print(f\"  Models: {len(config['models']['active'])}\")\n",
    "print(f\"  Imbalance methods: {len(config['imbalance']['methods'])}\")\n",
    "print(f\"  Parallel workers: {config['training']['n_workers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16b381",
   "metadata": {},
   "source": [
    "## 4. Create Run Folder and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create run folder\n",
    "run_folder = create_run_folder()\n",
    "log = []\n",
    "\n",
    "# Optional: create models folder if saving models\n",
    "if config['output']['save_models']:\n",
    "    os.makedirs(f\"{run_folder}/models\", exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Results will be saved to: {run_folder}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = config['dataset']['name']\n",
    "dataset_class = eval(f\"{dataset_name.title().replace('_', '')}Dataset\")\n",
    "\n",
    "dataset = dataset_class(\n",
    "    train_path=config['dataset'].get('train_path'),\n",
    "    test_path=config['dataset'].get('test_path')\n",
    ")\n",
    "\n",
    "X_train, y_train, X_test, y_test = dataset.load()\n",
    "\n",
    "log.append(f\"Loaded dataset: {dataset.name}\")\n",
    "log.append(f\"Task type: {dataset.task_type}\")\n",
    "log.append(f\"Training samples: {len(X_train)}\")\n",
    "log.append(f\"Test samples: {len(X_test)}\")\n",
    "log.append(f\"Features: {X_train.shape[1]}\")\n",
    "log.append(f\"Classes: {np.unique(y_train)}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset: {dataset.name}\")\n",
    "print(f\"  Task: {dataset.task_type}\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")\n",
    "print(f\"  Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf9151",
   "metadata": {},
   "source": [
    "## 5. Define Training Job Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba57623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_job(model_name, imbalance_method, X_train, y_train, X_test, y_test, config, dataset):\n",
    "    \"\"\"Train a single model with given imbalance method\"\"\"\n",
    "    job_log = []\n",
    "    job_name = f\"{model_name}_{imbalance_method}\"\n",
    "    \n",
    "    # Apply imbalance handling\n",
    "    imbalance_handler = ImbalanceHandler(\n",
    "        method=imbalance_method,\n",
    "        **config['imbalance']['params']\n",
    "    )\n",
    "    X_train_balanced, y_train_balanced = imbalance_handler.apply(\n",
    "        X_train.copy(), y_train.copy(), task_type=dataset.task_type\n",
    "    )\n",
    "    job_log.append(f\"Samples: {len(X_train)} -> {len(X_train_balanced)}\")\n",
    "    \n",
    "    # Create model\n",
    "    params = config['models'].get('params', {}).get(model_name, {})\n",
    "    model = ModelFactory.create_model(model_name, params)\n",
    "    \n",
    "    # Add scaler if needed\n",
    "    scaler = StandardScaler() if ModelFactory.requires_scaling(model_name) else None\n",
    "    \n",
    "    # Train\n",
    "    trainer = Trainer(model, model_name, scaler)\n",
    "    trainer.train(X_train_balanced, y_train_balanced)\n",
    "    job_log.append(f\"Training time: {trainer.train_time:.2f}s\")\n",
    "    \n",
    "    # Predict\n",
    "    if dataset.task_type == \"binary\":\n",
    "        y_pred_proba = trainer.predict_proba(X_test)\n",
    "    else:\n",
    "        # For multiclass, get full probability matrix\n",
    "        if scaler:\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "        else:\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_config = config['evaluation']\n",
    "    evaluator = Evaluator(\n",
    "        thresholds=eval_config['thresholds'],\n",
    "        task_type=dataset.task_type,\n",
    "        average=eval_config.get('multiclass', {}).get('average', 'macro')\n",
    "    )\n",
    "    metrics = evaluator.evaluate_model(y_test, y_pred_proba, model_name, imbalance_method, trainer.train_time)\n",
    "    \n",
    "    return {\n",
    "        'job_name': job_name,\n",
    "        'model_name': model_name,\n",
    "        'imbalance_method': imbalance_method,\n",
    "        'metrics': metrics,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'trainer': trainer,\n",
    "        'log': job_log\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training job function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8482b8",
   "metadata": {},
   "source": [
    "## 6. Generate Training Jobs and Train in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all training jobs (model x imbalance_method combinations)\n",
    "training_jobs = []\n",
    "for model_name in config['models']['active']:\n",
    "    for imbalance_method in config['imbalance']['methods']:\n",
    "        training_jobs.append((model_name, imbalance_method))\n",
    "\n",
    "print(f\"\\nüöÄ Starting parallel training...\")\n",
    "print(f\"  Total jobs: {len(training_jobs)}\")\n",
    "print(f\"  Workers: {config['training']['n_workers']}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "log.append(f\"\\nTotal training jobs: {len(training_jobs)}\")\n",
    "log.append(f\"Parallel workers: {config['training']['n_workers']}\")\n",
    "\n",
    "# Train all jobs in parallel\n",
    "results = Parallel(n_jobs=config['training']['n_workers'], verbose=10)(\n",
    "    delayed(train_single_job)(\n",
    "        model_name, imbalance_method, \n",
    "        X_train, y_train, X_test, y_test, \n",
    "        config, dataset\n",
    "    )\n",
    "    for model_name, imbalance_method in training_jobs\n",
    ")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n‚úÖ All training jobs completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2889d",
   "metadata": {},
   "source": [
    "## 7. Collect Results and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d44a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "results_dict = {}  # For visualization: {job_name: y_pred_proba}\n",
    "all_metrics = []\n",
    "\n",
    "for result in results:\n",
    "    job_name = result['job_name']\n",
    "    results_dict[job_name] = result['y_pred_proba']\n",
    "    all_metrics.extend(result['metrics'])\n",
    "    \n",
    "    log.append(f\"\\n{job_name}:\")\n",
    "    for msg in result['log']:\n",
    "        log.append(f\"  {msg}\")\n",
    "    \n",
    "    # Save model if configured\n",
    "    if config['output']['save_models']:\n",
    "        result['trainer'].save_model(f\"{run_folder}/models/{job_name}.pkl\")\n",
    "        log.append(f\"  Model saved\")\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "print(f\"üìä Results collected:\")\n",
    "print(f\"  Total models trained: {len(results)}\")\n",
    "print(f\"  Metrics computed: {len(all_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5232a5",
   "metadata": {},
   "source": [
    "## 8. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db68585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics table\n",
    "print(\"\\nüìã Performance Metrics (Threshold = 0.5):\\n\")\n",
    "\n",
    "if 'threshold' in metrics_df.columns:\n",
    "    display_df = metrics_df[metrics_df['threshold'] == 0.5].copy()\n",
    "else:\n",
    "    display_df = metrics_df.copy()\n",
    "\n",
    "display_df = display_df.sort_values('auc', ascending=False)\n",
    "display(display_df[['model', 'imbalance_method', 'accuracy', 'auc', 'f1', 'train_time']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bed1c",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bac6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = Visualizer()\n",
    "\n",
    "if config['output']['save_plots']:\n",
    "    if dataset.task_type == \"binary\":\n",
    "        # Binary classification plots\n",
    "        visualizer.plot_roc_curves(results_dict, y_test, f\"{run_folder}/plots/roc_curves.png\")\n",
    "        visualizer.plot_confusion_matrices(results_dict, y_test, 0.5, f\"{run_folder}/plots/confusion_matrices.png\")\n",
    "    else:\n",
    "        # Multi-class plots\n",
    "        visualizer.plot_multiclass_roc(results_dict, y_test, f\"{run_folder}/plots/roc_curves_multiclass.png\")\n",
    "    \n",
    "    # Metrics comparison (works for both binary and multiclass)\n",
    "    visualizer.plot_metrics_comparison(metrics_df, f\"{run_folder}/plots/metrics_comparison.png\")\n",
    "    \n",
    "    print(f\"\\nüìä Plots saved to {run_folder}/plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec362f4",
   "metadata": {},
   "source": [
    "## 10. Save Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f55406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "save_summary(run_folder, config, metrics_df, dataset.get_info())\n",
    "save_metrics(run_folder, metrics_df)\n",
    "save_training_log(run_folder, log)\n",
    "\n",
    "# Print summary\n",
    "best_idx = metrics_df['auc'].idxmax()\n",
    "best_model = metrics_df.loc[best_idx, 'model']\n",
    "best_method = metrics_df.loc[best_idx, 'imbalance_method']\n",
    "best_auc = metrics_df.loc[best_idx, 'auc']\n",
    "best_acc = metrics_df.loc[best_idx, 'accuracy']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ Run Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nüìÅ Results saved to: {run_folder}\")\n",
    "print(f\"\\nüèÜ Best Model:\")\n",
    "print(f\"  - Model: {best_model}\")\n",
    "print(f\"  - Imbalance method: {best_method}\")\n",
    "print(f\"  - AUC: {best_auc:.4f}\")\n",
    "print(f\"  - Accuracy: {best_acc:.4f}\")\n",
    "print(f\"\\nüìÑ Files created:\")\n",
    "print(f\"  - summary.json (quick overview)\")\n",
    "print(f\"  - metrics.csv (detailed metrics)\")\n",
    "print(f\"  - training.log (execution logs)\")\n",
    "if config['output']['save_plots']:\n",
    "    print(f\"  - plots/ (visualizations)\")\n",
    "if config['output']['save_models']:\n",
    "    print(f\"  - models/ (trained models)\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f0e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
