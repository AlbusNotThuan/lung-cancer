{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125fffe1",
   "metadata": {},
   "source": [
    "## ðŸš€ Google Colab Setup\n",
    "\n",
    "**Run these commands in Colab before running the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\n",
    "%cd YOUR_REPO_NAME\n",
    "\n",
    "# Install package in development mode\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531daa8",
   "metadata": {},
   "source": [
    "# Active Learning Training Pipeline\n",
    "\n",
    "This notebook implements **pseudo-labeling with active learning** for lung cancer survival prediction.\n",
    "\n",
    "## Two Approaches:\n",
    "1. **Combined**: Randomly select labeled ratio from both train and test sets combined\n",
    "2. **Train Only**: Apply active learning on training set, evaluate on separate test set\n",
    "\n",
    "## Key Features:\n",
    "- Progressive confidence threshold (starts low, increases over epochs)\n",
    "- Entropy-based confidence scoring\n",
    "- Imbalance handling applied only to initial labeled data\n",
    "- Parallel training across all models Ã— imbalance methods\n",
    "- Comprehensive metrics tracking per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54c529",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom modules\n",
    "from datasets.lung_cancer import LungCancerDataset\n",
    "from modules.imbalance_handler import ImbalanceHandler\n",
    "from modules.models import ModelFactory\n",
    "from modules.trainer import Trainer\n",
    "from modules.evaluator import Evaluator\n",
    "from modules.visualizer import Visualizer\n",
    "from modules.active_learning import (\n",
    "    active_learning_cycle,\n",
    "    split_data_combined,\n",
    "    split_data_train_only\n",
    ")\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66aee6",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract settings\n",
    "RANDOM_SEED = config['random_seed']\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Active learning parameters\n",
    "al_config = config['active_learning']\n",
    "APPROACH = al_config['approach']  # \"combined\" or \"train_only\"\n",
    "LABELED_RATIO = al_config['labeled_ratio']\n",
    "VALIDATION_RATIO = al_config['validation_ratio']\n",
    "CONFIDENCE_THRESHOLD = al_config['confidence_threshold']\n",
    "USE_DYNAMIC_THRESHOLD = al_config['use_dynamic_threshold']\n",
    "TAU_MIN = al_config['tau_min']\n",
    "TAU_MAX = al_config['tau_max']\n",
    "MAX_EPOCHS = al_config['max_epochs']\n",
    "APPLY_IMBALANCE_TO_INITIAL = al_config['apply_imbalance_to_initial']\n",
    "\n",
    "# Model and imbalance settings\n",
    "ACTIVE_MODELS = config['models']['active']\n",
    "IMBALANCE_METHODS = config['imbalance']['methods']\n",
    "N_WORKERS = config['training']['n_workers']\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Approach: {APPROACH}\")\n",
    "print(f\"  Labeled Ratio: {LABELED_RATIO:.1%}\")\n",
    "print(f\"  Validation Ratio: {VALIDATION_RATIO:.1%}\")\n",
    "print(f\"  Confidence Threshold: {CONFIDENCE_THRESHOLD} (Dynamic: {USE_DYNAMIC_THRESHOLD})\")\n",
    "print(f\"  Max Epochs: {MAX_EPOCHS}\")\n",
    "print(f\"  Models: {len(ACTIVE_MODELS)}\")\n",
    "print(f\"  Imbalance Methods: {len(IMBALANCE_METHODS)}\")\n",
    "print(f\"  Parallel Workers: {N_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39edef",
   "metadata": {},
   "source": [
    "## 3. Load Dataset and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8140cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = LungCancerDataset()\n",
    "dataset_info = dataset.get_info()\n",
    "X_train_orig, y_train_orig, X_test_orig, y_test_orig = dataset.load()\n",
    "\n",
    "print(f\"Dataset: {dataset_info['name']}\")\n",
    "print(f\"  Description: {dataset_info['description']}\")\n",
    "print(f\"  Task Type: {dataset_info['task_type']}\")\n",
    "print(f\"  Original Train: {X_train_orig.shape}\")\n",
    "print(f\"  Original Test: {X_test_orig.shape}\")\n",
    "print()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_orig = X_train_orig.values\n",
    "y_train_orig = y_train_orig.values\n",
    "X_test_orig = X_test_orig.values\n",
    "y_test_orig = y_test_orig.values\n",
    "\n",
    "# Split data based on approach\n",
    "if APPROACH == \"combined\":\n",
    "    print(f\"=== Approach 1: Combined Dataset Split ===\")\n",
    "    X_labeled, y_labeled, X_val, y_val, X_unlabeled, y_unlabeled = split_data_combined(\n",
    "        X_train_orig, y_train_orig, X_test_orig, y_test_orig,\n",
    "        labeled_ratio=LABELED_RATIO,\n",
    "        validation_ratio=VALIDATION_RATIO,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    X_test_final = None\n",
    "    y_test_final = None\n",
    "    \n",
    "    print(f\"  Initial Labeled: {X_labeled.shape[0]} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "    print(f\"  Unlabeled Pool: {X_unlabeled.shape[0]} samples\")\n",
    "    print(f\"  (No separate test set - unlabeled pool serves as test data)\")\n",
    "    \n",
    "elif APPROACH == \"train_only\":\n",
    "    print(f\"=== Approach 2: Train-Only Split ===\")\n",
    "    X_labeled, y_labeled, X_val, y_val, X_unlabeled, y_unlabeled, X_test_final, y_test_final = split_data_train_only(\n",
    "        X_train_orig, y_train_orig, X_test_orig, y_test_orig,\n",
    "        labeled_ratio=LABELED_RATIO,\n",
    "        validation_ratio=VALIDATION_RATIO,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"  Initial Labeled: {X_labeled.shape[0]} samples\")\n",
    "    print(f\"  Validation: {X_val.shape[0]} samples\")\n",
    "    print(f\"  Unlabeled Pool: {X_unlabeled.shape[0]} samples\")\n",
    "    print(f\"  Final Test Set: {X_test_final.shape[0]} samples\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown approach: {APPROACH}\")\n",
    "\n",
    "print(f\"\\nClass distribution in initial labeled data: {np.bincount(y_labeled.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa18251",
   "metadata": {},
   "source": [
    "## 4. Define Training Job Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_active_learning_job(model_name, imbalance_method, \n",
    "                               X_labeled_init, y_labeled_init,\n",
    "                               X_val, y_val,\n",
    "                               X_unlabeled, y_unlabeled,\n",
    "                               X_test_final, y_test_final,\n",
    "                               config):\n",
    "    \"\"\"\n",
    "    Train a single model using active learning\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model name, imbalance method, metrics history, and final results\n",
    "    \"\"\"\n",
    "    job_start = time.time()\n",
    "    \n",
    "    # Copy data to avoid modifying originals\n",
    "    X_labeled = X_labeled_init.copy()\n",
    "    y_labeled = y_labeled_init.copy()\n",
    "    X_unlab = X_unlabeled.copy()\n",
    "    y_unlab = y_unlabeled.copy()\n",
    "    \n",
    "    # Apply imbalance handling to initial labeled data\n",
    "    if imbalance_method != \"none\" and APPLY_IMBALANCE_TO_INITIAL:\n",
    "        imbalance_handler = ImbalanceHandler(imbalance_method, random_state=RANDOM_SEED)\n",
    "        X_labeled, y_labeled = imbalance_handler.apply(\n",
    "            X_labeled, y_labeled, \n",
    "            task_type=dataset_info['task_type']\n",
    "        )\n",
    "    \n",
    "    # Create and train initial model\n",
    "    model_factory = ModelFactory(config)\n",
    "    model = model_factory.create_model(model_name)\n",
    "    trainer = Trainer(model, model_name, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Train initial model\n",
    "    trainer.train(X_labeled, y_labeled)\n",
    "    \n",
    "    # Track metrics per epoch\n",
    "    metrics_history = []\n",
    "    \n",
    "    # Initial evaluation\n",
    "    evaluator = Evaluator()\n",
    "    initial_val_metrics = evaluator.evaluate_model(\n",
    "        trainer.model, X_val, y_val,\n",
    "        task_type=dataset_info['task_type']\n",
    "    )\n",
    "    \n",
    "    metrics_history.append({\n",
    "        'epoch': 0,\n",
    "        'threshold': 0.0,\n",
    "        'pseudo_labeled': 0,\n",
    "        'labeled_size': len(X_labeled_init),\n",
    "        'unlabeled_remaining': len(X_unlab),\n",
    "        'val_acc': initial_val_metrics['accuracy'],\n",
    "        'val_auc': initial_val_metrics.get('auc', 0.0)\n",
    "    })\n",
    "    \n",
    "    # Active learning cycles\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        model_updated, X_labeled, y_labeled, X_unlab, y_unlab, epoch_metrics = active_learning_cycle(\n",
    "            model=trainer.model,\n",
    "            X_labeled=X_labeled,\n",
    "            y_labeled=y_labeled,\n",
    "            X_unlabeled=X_unlab,\n",
    "            y_unlabeled=y_unlab,\n",
    "            X_val=X_val,\n",
    "            y_val=y_val,\n",
    "            epoch=epoch,\n",
    "            total_epochs=MAX_EPOCHS,\n",
    "            use_dynamic_threshold=USE_DYNAMIC_THRESHOLD,\n",
    "            confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    "            tau_min=TAU_MIN,\n",
    "            tau_max=TAU_MAX,\n",
    "            verbose=False  # Suppress per-job output\n",
    "        )\n",
    "        \n",
    "        # Update trainer's model\n",
    "        trainer.model = model_updated\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_metrics['labeled_size'] = len(X_labeled)\n",
    "        epoch_metrics['val_auc'] = evaluator.evaluate_model(\n",
    "            trainer.model, X_val, y_val,\n",
    "            task_type=dataset_info['task_type']\n",
    "        ).get('auc', 0.0)\n",
    "        \n",
    "        metrics_history.append(epoch_metrics)\n",
    "        \n",
    "        # Stop if no more unlabeled data\n",
    "        if len(X_unlab) == 0:\n",
    "            break\n",
    "    \n",
    "    # Final evaluation\n",
    "    if X_test_final is not None:\n",
    "        # Approach 2: Evaluate on separate test set\n",
    "        final_metrics = evaluator.evaluate_model(\n",
    "            trainer.model, X_test_final, y_test_final,\n",
    "            task_type=dataset_info['task_type']\n",
    "        )\n",
    "        eval_set = \"test\"\n",
    "    else:\n",
    "        # Approach 1: Evaluate on validation set (no separate test)\n",
    "        final_metrics = evaluator.evaluate_model(\n",
    "            trainer.model, X_val, y_val,\n",
    "            task_type=dataset_info['task_type']\n",
    "        )\n",
    "        eval_set = \"validation\"\n",
    "    \n",
    "    job_time = time.time() - job_start\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'imbalance_method': imbalance_method,\n",
    "        'metrics_history': pd.DataFrame(metrics_history),\n",
    "        'final_metrics': final_metrics,\n",
    "        'eval_set': eval_set,\n",
    "        'training_time': job_time,\n",
    "        'final_labeled_size': len(X_labeled),\n",
    "        'total_epochs': len(metrics_history) - 1\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Training job function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5379a05",
   "metadata": {},
   "source": [
    "## 5. Run Parallel Active Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create job list\n",
    "jobs = [\n",
    "    (model_name, imbalance_method)\n",
    "    for model_name in ACTIVE_MODELS\n",
    "    for imbalance_method in IMBALANCE_METHODS\n",
    "]\n",
    "\n",
    "print(f\"Starting {len(jobs)} active learning training jobs with {N_WORKERS} workers...\")\n",
    "print(f\"Models: {ACTIVE_MODELS}\")\n",
    "print(f\"Imbalance Methods: {IMBALANCE_METHODS}\")\n",
    "print()\n",
    "\n",
    "# Run parallel training\n",
    "training_start = time.time()\n",
    "\n",
    "results = Parallel(n_jobs=N_WORKERS, verbose=10)(\n",
    "    delayed(train_active_learning_job)(\n",
    "        model_name, imbalance_method,\n",
    "        X_labeled, y_labeled,\n",
    "        X_val, y_val,\n",
    "        X_unlabeled, y_unlabeled,\n",
    "        X_test_final, y_test_final,\n",
    "        config\n",
    "    )\n",
    "    for model_name, imbalance_method in jobs\n",
    ")\n",
    "\n",
    "total_training_time = time.time() - training_start\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ All {len(results)} jobs completed in {total_training_time:.2f}s\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc806514",
   "metadata": {},
   "source": [
    "## 6. Organize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ded3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for result in results:\n",
    "    summary_data.append({\n",
    "        'model': result['model_name'],\n",
    "        'imbalance': result['imbalance_method'],\n",
    "        'final_accuracy': result['final_metrics']['accuracy'],\n",
    "        'final_auc': result['final_metrics'].get('auc', 0.0),\n",
    "        'final_f1': result['final_metrics'].get('f1', 0.0),\n",
    "        'final_sensitivity': result['final_metrics'].get('sensitivity', 0.0),\n",
    "        'final_specificity': result['final_metrics'].get('specificity', 0.0),\n",
    "        'training_time': result['training_time'],\n",
    "        'total_epochs': result['total_epochs'],\n",
    "        'final_labeled_size': result['final_labeled_size'],\n",
    "        'eval_set': result['eval_set']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('final_auc', ascending=False)\n",
    "\n",
    "print(\"Top 10 Models by AUC:\")\n",
    "print(summary_df.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Best model\n",
    "best_result = results[summary_df.index[0]]\n",
    "print(f\"Best Model: {best_result['model_name']} + {best_result['imbalance_method']}\")\n",
    "print(f\"  Final AUC: {best_result['final_metrics']['auc']:.4f}\")\n",
    "print(f\"  Final Accuracy: {best_result['final_metrics']['accuracy']:.4f}\")\n",
    "print(f\"  Training Time: {best_result['training_time']:.2f}s\")\n",
    "print(f\"  Total Epochs: {best_result['total_epochs']}\")\n",
    "print(f\"  Final Labeled Size: {best_result['final_labeled_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f92f49",
   "metadata": {},
   "source": [
    "## 7. Visualize Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for top 5 models\n",
    "top_5_indices = summary_df.head(5).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result_idx in enumerate(top_5_indices):\n",
    "    result = results[result_idx]\n",
    "    history = result['metrics_history']\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot validation accuracy and AUC\n",
    "    ax.plot(history['epoch'], history['val_acc'], marker='o', label='Val Accuracy', linewidth=2)\n",
    "    ax.plot(history['epoch'], history['val_auc'], marker='s', label='Val AUC', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch', fontsize=10)\n",
    "    ax.set_ylabel('Score', fontsize=10)\n",
    "    ax.set_title(f\"{result['model_name']}\\n({result['imbalance_method']})\", fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "# Plot pseudo-label growth for best model\n",
    "best_result = results[summary_df.index[0]]\n",
    "best_history = best_result['metrics_history']\n",
    "\n",
    "ax = axes[5]\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "# Labeled size\n",
    "ax.plot(best_history['epoch'], best_history['labeled_size'], \n",
    "        marker='o', color='green', linewidth=2, label='Labeled Size')\n",
    "ax.set_xlabel('Epoch', fontsize=10)\n",
    "ax.set_ylabel('Labeled Size', fontsize=10, color='green')\n",
    "ax.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Unlabeled remaining\n",
    "ax2.plot(best_history['epoch'], best_history['unlabeled_remaining'], \n",
    "         marker='s', color='red', linewidth=2, label='Unlabeled Remaining')\n",
    "ax2.set_ylabel('Unlabeled Remaining', fontsize=10, color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax.set_title(f\"Best Model: {best_result['model_name']}\\nData Growth\", \n",
    "             fontsize=11, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Learning curves plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28539aa1",
   "metadata": {},
   "source": [
    "## 8. Compare Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc341bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models by imbalance method\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_to_plot = ['final_auc', 'final_accuracy', 'final_f1']\n",
    "titles = ['AUC', 'Accuracy', 'F1-Score']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Group by imbalance method\n",
    "    for imbalance in IMBALANCE_METHODS:\n",
    "        subset = summary_df[summary_df['imbalance'] == imbalance]\n",
    "        ax.scatter(range(len(subset)), subset[metric], \n",
    "                  label=imbalance, alpha=0.7, s=100)\n",
    "    \n",
    "    ax.set_xlabel('Model Index', fontsize=11)\n",
    "    ax.set_ylabel(title, fontsize=11)\n",
    "    ax.set_title(f'Final {title} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of top 10 models\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "top_10 = summary_df.head(10).copy()\n",
    "top_10['label'] = top_10['model'] + '\\n(' + top_10['imbalance'] + ')'\n",
    "\n",
    "x_pos = np.arange(len(top_10))\n",
    "ax.bar(x_pos, top_10['final_auc'], alpha=0.7, color='steelblue')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(top_10['label'], rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('AUC', fontsize=11)\n",
    "ax.set_title('Top 10 Models by AUC', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Performance comparison plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1a1c2",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = f\"results/active_learning_{APPROACH}_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(f\"{results_dir}/summary.csv\", index=False)\n",
    "print(f\"âœ“ Summary saved to {results_dir}/summary.csv\")\n",
    "\n",
    "# Save detailed metrics history for each model\n",
    "for result in results:\n",
    "    model_name = result['model_name']\n",
    "    imbalance = result['imbalance_method']\n",
    "    filename = f\"{results_dir}/history_{model_name}_{imbalance}.csv\"\n",
    "    result['metrics_history'].to_csv(filename, index=False)\n",
    "\n",
    "print(f\"âœ“ {len(results)} detailed history files saved\")\n",
    "\n",
    "# Save experiment configuration\n",
    "config_summary = {\n",
    "    'approach': APPROACH,\n",
    "    'labeled_ratio': LABELED_RATIO,\n",
    "    'validation_ratio': VALIDATION_RATIO,\n",
    "    'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "    'use_dynamic_threshold': USE_DYNAMIC_THRESHOLD,\n",
    "    'tau_min': TAU_MIN,\n",
    "    'tau_max': TAU_MAX,\n",
    "    'max_epochs': MAX_EPOCHS,\n",
    "    'initial_labeled': len(X_labeled),\n",
    "    'validation_size': len(X_val),\n",
    "    'unlabeled_pool': len(X_unlabeled),\n",
    "    'test_size': len(X_test_final) if X_test_final is not None else 0,\n",
    "    'total_training_time': total_training_time,\n",
    "    'n_models': len(ACTIVE_MODELS),\n",
    "    'n_imbalance_methods': len(IMBALANCE_METHODS),\n",
    "    'n_workers': N_WORKERS\n",
    "}\n",
    "\n",
    "with open(f\"{results_dir}/config_summary.yaml\", 'w') as f:\n",
    "    yaml.dump(config_summary, f, default_flow_style=False)\n",
    "\n",
    "print(f\"âœ“ Configuration saved to {results_dir}/config_summary.yaml\")\n",
    "print()\n",
    "print(f\"All results saved to: {results_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2dc44",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d433ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" ACTIVE LEARNING TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"Approach: {APPROACH}\")\n",
    "print(f\"  - {'Combined train+test split' if APPROACH == 'combined' else 'Train-only split with separate test set'}\")\n",
    "print()\n",
    "print(f\"Dataset Configuration:\")\n",
    "print(f\"  Initial Labeled: {len(X_labeled)} samples ({LABELED_RATIO:.1%})\")\n",
    "print(f\"  Validation: {len(X_val)} samples ({VALIDATION_RATIO:.1%} of labeled)\")\n",
    "print(f\"  Unlabeled Pool: {len(X_unlabeled)} samples\")\n",
    "if X_test_final is not None:\n",
    "    print(f\"  Final Test Set: {len(X_test_final)} samples\")\n",
    "print()\n",
    "print(f\"Active Learning Configuration:\")\n",
    "print(f\"  Max Epochs: {MAX_EPOCHS}\")\n",
    "print(f\"  Dynamic Threshold: {USE_DYNAMIC_THRESHOLD}\")\n",
    "if USE_DYNAMIC_THRESHOLD:\n",
    "    print(f\"    - Range: {TAU_MIN:.2f} â†’ {TAU_MAX:.2f}\")\n",
    "else:\n",
    "    print(f\"    - Static: {CONFIDENCE_THRESHOLD:.2f}\")\n",
    "print(f\"  Imbalance on Initial: {APPLY_IMBALANCE_TO_INITIAL}\")\n",
    "print()\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Models Tested: {len(ACTIVE_MODELS)}\")\n",
    "print(f\"  Imbalance Methods: {len(IMBALANCE_METHODS)}\")\n",
    "print(f\"  Total Jobs: {len(results)}\")\n",
    "print(f\"  Parallel Workers: {N_WORKERS}\")\n",
    "print(f\"  Total Training Time: {total_training_time:.2f}s ({total_training_time/60:.1f}m)\")\n",
    "print()\n",
    "print(f\"Top 5 Models:\")\n",
    "for i, (idx, row) in enumerate(summary_df.head(5).iterrows(), 1):\n",
    "    result = results[idx]\n",
    "    print(f\"  {i}. {row['model']:20s} + {row['imbalance']:15s} | \"\n",
    "          f\"AUC: {row['final_auc']:.4f} | Acc: {row['final_accuracy']:.4f} | \"\n",
    "          f\"F1: {row['final_f1']:.4f} | Time: {row['training_time']:.1f}s\")\n",
    "print()\n",
    "print(f\"Best Model Details:\")\n",
    "best_result = results[summary_df.index[0]]\n",
    "print(f\"  Model: {best_result['model_name']}\")\n",
    "print(f\"  Imbalance Method: {best_result['imbalance_method']}\")\n",
    "print(f\"  Final Metrics (on {best_result['eval_set']} set):\")\n",
    "for metric, value in best_result['final_metrics'].items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"    - {metric}: {value:.4f}\")\n",
    "print(f\"  Training Statistics:\")\n",
    "print(f\"    - Total Epochs: {best_result['total_epochs']}\")\n",
    "print(f\"    - Final Labeled Size: {best_result['final_labeled_size']}\")\n",
    "print(f\"    - Training Time: {best_result['training_time']:.2f}s\")\n",
    "print()\n",
    "print(f\"Results saved to: {results_dir}/\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
